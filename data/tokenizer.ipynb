{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "DATASET = '240212_plasmid_seq_54646.fasta'\n",
    "DATASET_TXT = '240212_plasmid_seq_54646.txt'\n",
    "DATASET_DUMMY = '240212_plasmid_seq_54646_dummy.txt'\n",
    "TOKENIZER = 'dna_bpe_tokenizer'\n",
    "\n",
    "VOCAB_SIZE = 4096\n",
    "NUM_SEQUENCES = 10     #54646\n",
    "MAX_TOKEN_LENGTH = 32\n",
    "SPECIAL_TOKENS = ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta_to_dataframe(file_path):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequences.append([record.id, str(record.seq), record.description])\n",
    "    \n",
    "    dataset = pd.DataFrame(sequences, columns=['ID', 'Sequence', 'Description'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Read plasmid dataset\n",
    "dataset = read_fasta_to_dataframe(DATASET)\n",
    "\n",
    "# Remove new line and space characters from DNA sequences\n",
    "dataset['Sequence'] = dataset['Sequence'].transform(\n",
    "    lambda seq: seq.replace(' ', '').replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    ")\n",
    "\n",
    "# Save dataset to txt for tokenizer\n",
    "# dataset['Sequence'].to_csv(DATASET_TXT, index=False, header=False)\n",
    "dataset['Sequence'].iloc[:NUM_SEQUENCES].to_csv(DATASET_DUMMY, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SentencePiece model with HuggingFace\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token=\"[UNK]\")\n",
    ")\n",
    "\n",
    "# Set pre-tokenizer\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Train tokenizer\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=SPECIAL_TOKENS,\n",
    "    initial_alphabet=[\"A\", \"T\", \"C\", \"G\"],\n",
    "    max_token_length=MAX_TOKEN_LENGTH\n",
    ")\n",
    "\n",
    "tokenizer.train([DATASET_DUMMY], trainer)\n",
    "\n",
    "# Set post-processor with correct special token references\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[PAD]\", tokenizer.token_to_id(\"[PAD]\")),\n",
    "        (\"[UNK]\", tokenizer.token_to_id(\"[UNK]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer.save(f\"{TOKENIZER}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SentencePiece tokenizer\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=f\"{TOKENIZER}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sequence: {'input_ids': [3, 5, 532, 711, 40, 133, 86, 2202, 530, 41, 229, 132, 133, 269, 27, 1094, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Decoded sequence: [CLS] A TTCTGC GGTTCC CCC TGGAA GACC TACGCAA GTTGG GCCA GCTCA GAGG TGGAA TCAAC GAA GGCGAGC [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer on a sample DNA sequence\n",
    "sequence = \"ATTCTGCGGTTCCCCCTGGAAGACCTACGCAAGTTGGGCCAGCTCAGAGGTGGAATCAACGAAGGCGAGC\"\n",
    "encoded = tokenizer(sequence)\n",
    "print(\"Encoded sequence:\", encoded)\n",
    "\n",
    "# Decode the tokens back to the original sequence\n",
    "decoded_sequence = tokenizer.decode(encoded['input_ids'])\n",
    "print(\"Decoded sequence:\", decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### DEAD ZONE - DO NOT ENTER ####################\n",
    "# dataset['Sequence'].transform(len).plot(kind='hist', bins=int(1e3))\n",
    "# plt.xlim(0, 1e6)\n",
    "# plt.show()\n",
    "\n",
    "# # Train the SentencePiece model with spm\n",
    "\n",
    "# with tqdm(total=NUM_SEQUENCES, desc=\"Training Tokenizer...\", unit=\"sequences\") as pbar:\n",
    "\n",
    "#     spm.SentencePieceTrainer.train(\n",
    "#         input=DATASET_TXT,                      # Input file containing DNA sequences\n",
    "#         model_prefix=TOKENIZER,                 # Prefix for the output model files\n",
    "#         vocab_size=VOCAB_SIZE,                  # Vocabulary size\n",
    "#         model_type='bpe',                       # Model type (BPE)\n",
    "#         pad_id=0,                               # ID for padding token\n",
    "#         unk_id=1,                               # ID for unknown token\n",
    "#         bos_id=2,                               # ID for beginning-of-sequence token\n",
    "#         eos_id=3,                               # ID for end-of-sequence token\n",
    "#         user_defined_symbols=SPECIAL_TOKENS,    # Special tokens\n",
    "#         character_coverage=1.0,                 # Ensure full coverage of the input characters\n",
    "#         input_sentence_size=NUM_SEQUENCES,      # Limit the number of sentences for training for efficiency\n",
    "#         shuffle_input_sentence=True             # Shuffle the input sentences to improve training\n",
    "#     )\n",
    "#     pbar.update(NUM_SEQUENCES)\n",
    "\n",
    "# # Load the SentencePiece model\n",
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.load(f'{TOKENIZER}.model')\n",
    "\n",
    "# # Tokenize a sentence\n",
    "# sequence = \"ATTCTGCGGTTCCCCCTGGAAGACCTACGCAAGTTGGGCCAGCTCAGAGGTGGAATCAACGAAGGCGAGC\"\n",
    "# tokens = sp.encode_as_pieces(sequence)\n",
    "# print(\"Tokens:\", tokens)\n",
    "\n",
    "# # Convert tokens back to text\n",
    "# decoded_text = sp.decode_pieces(tokens)\n",
    "# print(\"Decoded text:\", decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "light",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

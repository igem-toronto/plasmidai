{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.normalizers import Normalizer, Replace, Lowercase\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "LETTER_TO_BASES = {\n",
    "    \"A\": \"A\",\n",
    "    \"B\": \"CGT\",\n",
    "    \"C\": \"C\",\n",
    "    \"D\": \"AGT\",\n",
    "    \"G\": \"G\",\n",
    "    \"H\": \"ACT\",\n",
    "    \"K\": \"GT\",\n",
    "    \"M\": \"AC\",\n",
    "    \"N\": \"ACGT\",\n",
    "    \"R\": \"AG\",\n",
    "    \"S\": \"CG\",\n",
    "    \"T\": \"T\",\n",
    "    \"V\": \"ACG\",\n",
    "    \"W\": \"AT\",\n",
    "    \"Y\": \"CT\",\n",
    "}\n",
    "\n",
    "ROOT = 'e:/PlasmidAI' # '/scratch/adibvafa/plasmid-ai/'\n",
    "DATA_ROOT = f'{ROOT}/data'\n",
    "DATA_SPLITS = f'{DATA_ROOT}/splits.csv'\n",
    "BASE_FILE = f'plasmids_replicon'\n",
    "DATASET = f'{DATA_ROOT}/{BASE_FILE}.fasta'\n",
    "DATASET_TXT = f'{DATA_ROOT}/{BASE_FILE}.txt'\n",
    "DATASET_CUTOFF = f'{DATA_ROOT}/{BASE_FILE}_cutoff.txt'\n",
    "DATASET_DUMMY =f'{DATA_ROOT}/{BASE_FILE}_dummy.txt'\n",
    "DATASET_FINETUNE = f'{DATA_ROOT}/{BASE_FILE}_finetune.txt'\n",
    "DATASET_CUTOFF_RC = f'{DATA_ROOT}/{BASE_FILE}_cutoff_rc.txt'\n",
    "TOKENIZER = 'dna_bpe_tokenizer_offset.json'\n",
    "\n",
    "SEED = 42\n",
    "LEN_CUTOFF = 100_000\n",
    "VOCAB_SIZE = 4096\n",
    "NUM_SEQUENCES = 10     #54646\n",
    "MAX_TOKEN_LENGTH = 32\n",
    "SPECIAL_TOKENS = ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta_to_dataframe(file_path):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequences.append([record.id, str(record.seq), record.description])\n",
    "    \n",
    "    dataset = pd.DataFrame(sequences, columns=['ID', 'Sequence', 'Description'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocess_dna_seqeunce(seq):\n",
    "    # Clean all whitespaces\n",
    "    seq = seq.upper()\n",
    "    cleaned_seq = seq.replace(' ', '').replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "    \n",
    "    # Replace each letter with a random base from LETTER_TO_BASES using random.randint\n",
    "    replaced_seq = ''\n",
    "    for letter in cleaned_seq:\n",
    "        bases = LETTER_TO_BASES[letter]\n",
    "        random_base = bases[random.randint(0, len(bases) - 1)]\n",
    "        replaced_seq += random_base\n",
    "    \n",
    "    return replaced_seq\n",
    "\n",
    "\n",
    "def reverse_compliment(dna):\n",
    "    return dna.translate(str.maketrans(\"ATCG\", \"TAGC\"))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read plasmid dataset and splits\n",
    "dataset = read_fasta_to_dataframe(DATASET)\n",
    "# dataset['Sequence'] = dataset['Sequence'].apply(preprocess_dna_seqeunce)\n",
    "\n",
    "# Select the finetune dataset\n",
    "data_splits = pd.read_csv(DATA_SPLITS)\n",
    "finetune_split = data_splits[(data_splits['split'] == 'train') & (data_splits['finetune'] == 1)]\n",
    "finetune_dataset = dataset[dataset['ID'].isin(finetune_split['id'])]\n",
    "\n",
    "# Select the cutoff dataset, whose length is less than LEN_CUTOFF\n",
    "cutoff_dataset = dataset[dataset['Sequence'].apply(lambda x: len(x) < LEN_CUTOFF)]\n",
    "\n",
    "# Add reverse compliment to the dataset\n",
    "cutoff_dataset['Sequence'] = cutoff_dataset['Sequence'].apply(preprocess_dna_seqeunce)\n",
    "reverse_compliment_dataset = cutoff_dataset.copy()\n",
    "reverse_compliment_dataset['Sequence'] = reverse_compliment_dataset['Sequence'].apply(reverse_compliment)\n",
    "cutoff_dataset_with_reverse_compliment = pd.concat([cutoff_dataset, reverse_compliment_dataset], axis=0)\n",
    "\n",
    "# Save dataset to txt for tokenizer\n",
    "dataset['Sequence'].to_csv(DATASET_TXT, index=False, header=False)\n",
    "# cutoff_dataset['Sequence'].to_csv(DATASET_CUTOFF, index=False, header=False)\n",
    "# finetune_dataset['Sequence'].to_csv(DATASET_FINETUNE, index=False, header=False)\n",
    "# dataset['Sequence'].iloc[:NUM_SEQUENCES].to_csv(DATASET_DUMMY, index=False, header=False)\n",
    "cutoff_dataset_with_reverse_compliment['Sequence'].to_csv(DATASET_CUTOFF_RC, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SentencePiece tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=TOKENIZER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer on a sample DNA sequence\n",
    "sequence = \"ATTCTGCGGTTCCCCCTGGAAGACCTACGCAAGTTGGGCCAGCTCAGAGGTGGAATCAACGAAGGCGAGC\"\n",
    "encoded = tokenizer(sequence)\n",
    "print(\"Encoded sequence:\", encoded)\n",
    "\n",
    "# Decode the tokens back to the original sequence\n",
    "decoded_sequence = tokenizer.decode(encoded['input_ids'])\n",
    "print(\"Decoded sequence:\", decoded_sequence.upper())\n",
    "\n",
    "# Anlyze the vocabulary\n",
    "print(f'Alphabete: {set(''.join(list(sorted(tokenizer.vocab.keys())[::-1])[5:]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### DEAD ZONE - DO NOT ENTER ####################\n",
    "# dataset['Sequence'].transform(len).plot(kind='hist', bins=int(1e3))\n",
    "# plt.xlim(0, 1e6)\n",
    "# plt.show()\n",
    "\n",
    "# # Train the SentencePiece model with spm\n",
    "\n",
    "# with tqdm(total=NUM_SEQUENCES, desc=\"Training Tokenizer...\", unit=\"sequences\") as pbar:\n",
    "\n",
    "#     spm.SentencePieceTrainer.train(\n",
    "#         input=DATASET_TXT,                      # Input file containing DNA sequences\n",
    "#         model_prefix=TOKENIZER,                 # Prefix for the output model files\n",
    "#         vocab_size=VOCAB_SIZE,                  # Vocabulary size\n",
    "#         model_type='bpe',                       # Model type (BPE)\n",
    "#         pad_id=0,                               # ID for padding token\n",
    "#         unk_id=1,                               # ID for unknown token\n",
    "#         bos_id=2,                               # ID for beginning-of-sequence token\n",
    "#         eos_id=3,                               # ID for end-of-sequence token\n",
    "#         user_defined_symbols=SPECIAL_TOKENS,    # Special tokens\n",
    "#         character_coverage=1.0,                 # Ensure full coverage of the input characters\n",
    "#         input_sentence_size=NUM_SEQUENCES,      # Limit the number of sentences for training for efficiency\n",
    "#         shuffle_input_sentence=True             # Shuffle the input sentences to improve training\n",
    "#     )\n",
    "#     pbar.update(NUM_SEQUENCES)\n",
    "\n",
    "# # Load the SentencePiece model\n",
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.load(f'{TOKENIZER}.model')\n",
    "\n",
    "# # Tokenize a sentence\n",
    "# sequence = \"ATTCTGCGGTTCCCCCTGGAAGACCTACGCAAGTTGGGCCAGCTCAGAGGTGGAATCAACGAAGGCGAGC\"\n",
    "# tokens = sp.encode_as_pieces(sequence)\n",
    "# print(\"Tokens:\", tokens)\n",
    "\n",
    "# # Convert tokens back to text\n",
    "# decoded_text = sp.decode_pieces(tokens)\n",
    "# print(\"Decoded text:\", decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
